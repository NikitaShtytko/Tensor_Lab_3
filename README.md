Лабораторная работа №3
====
# Цель лабораторной работы
Исследовать влияние параметра “темп обучения” на процесс обучения нейронной сети на примере решения задачи классификации Food-101 с использованием техники обучения Transfer Learning

# 1. С использованием техники обучения Transfer Learning обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Food-101 с использованием фиксированных темпов обучения 0.01, 0.001, 0.0001 
 
* Изменение темпов обучения от 0.1 до 0.0001
```
#optimizer=tf.optimizers.Adam(lr=0.01)
#optimizer=tf.optimizers.Adam(lr=0.001)
#optimizer=tf.optimizers.Adam(lr=0.0001)
```

 ### Графики обучения для сети EfficientNet-B0 с фиксированными темпами обучения 0.01, 0.001, 0.0001:
 
* Оранжевый - темп 0.01 на тренировочной выборке 
   / Синий - темп 0.01 на валидационной выборке
   
* Красный - темп 0.001 на тренировочной выборке 
   / Голубой - темп 0.001 на валидационной выборке
   
* Розовый - темп 0.0001 на тренировочной выборке 
   / Зеленый - темп 0.0001 на валидационной выборке
  
**График метрики точности:*** 
<img src="./logs-adam/epoch_categorical_accuracy.svg">

**График функции потерь:*** 
<img src="./logs-adam/epoch_loss.svg">


### Вывод:
Смотря на график метрики точности видно, что лучшее значения, достигаются при использовании темпов обучения 0.001, равное 87.92%, и 0.1, равное 87.32% . Смотря на график функции потерь видно, что минимальные потери были при темпах обучения 0.001 и 0.0001. Исходя из данных результатов можно сказать, что темп обучения 0.001 является оптимальным.

Согласно полученным графикам наибольшая точность на обучающей выборке у графика с темпом равным 0.001, порядка 84%. Однако наибольшая точность на валидационных данных достигается зеленым графиком, который соответствует темпу 0.0001 с максимальной точностью около 67%. На графиках функции потерь, зеленый график так же показывает лучший результат. Он же является единственным невозрастающим, поэтому можно говорить об отстутствии переобучения. Исходя из этого, я считаю, что темп равный 0.0001 является оптимальным.


# 2. Реализовать и применить в обучении следующие политики изменения темпаобучения, а также определить оптимальные параметры для каждой политики: 
* Косинусное затухание (Cosine Decay)
* Косинусное затухание с перезапусками (Cosine Decay with Restarts)

**Немного воды** 

### Косинусное затухание
 
```
tf.keras.experimental.CosineDecay(
    initial_learning_rate, decay_steps, alpha=0.0, name=None
)
```
* initial_learning_rate эксперементально подбирался (0.01, 0.001, 0.0001)
* decay_steps был взят как в [примере](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay) равным 1000
* alpha - ограничение шагов, взят равным 0


## Графики обучения сети EfficientNet-B0 с различными темпами обучения
    
* Красный - темп 0.01 на тренировочной выборке 
   / Голубой - темп 0.01 на валидационной выборке
   
* Оранжевый - темп 0.001 на тренировочной выборке 
   / Синий - темп 0.001 на валидационной выборке
   
* Розовый - темп 0.0001 на тренировочной выборке 
   / Зеленый - темп 0.0001 на валидационной выборке

**График метрики точности:*** 
<img src="./logs-cos/epoch_categorical_accuracy_step.svg">

**График функции потерь:*** 
<img src="./logs-cos/epoch_loss_step.svg">

## Вывод:
Лучшее значение точности достигается оранжевым графиком с темпом 0.001 с точностью около 84%. Однако в случае валидационных данных, лучший результат у темпа 0.0001 (зеленый график), точность около 67%. В данном случае здесь как и в предыдущем эксперименте лучшее на мой взгялд результат показал темп 0.0001.

### Косинусное затухание с перезапусками

```
tf.keras.experimental.CosineDecayRestarts(
    initial_learning_rate, first_decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,
    name=None
)
```
* initial_learning_rate исходя из предыдущих опытов был взят равным 0.0001
* decay_steps был взят как в [примере](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecayRestarts) равным 1000
* alpha - ограничение шагов, взят равным 0
* t_mul - количество итераций на i-том периоде
* m_mul - начальное обучабщее ограничение на i-том периоде

## Графики обучения сети EfficientNet-B0
 
* Оранжевый - на тренировочной выборке 
   / Синий - на валидационной выборке
 
**График метрики точности:*** 
<img src="./logs-cos-restart/epoch_categorical_accuracy_exp.svg">

**График функции потерь::*** 
<img src="./logs-cos-restart/epoch_loss_exp.svg">

### Анализ результатов:
Смотря на график метрики точности и функции потерь видно, что лучшее значения, достигаются при использовании параметров initial_lrate = 0.01 и k = 0.25. Получилось значение точности равное 89.07%, что на 1.15% лучше, чем при фиксированном темпе обучение. 


 ### Итоговое сравнение оптимальных результатов
 

**График метрики точности:*** 
<img src="./graph/epoch_categorical_accuracy_opt.svg">

**График функции потерь::*** 
 
<img src="./graph/epoch_loss_opt.svg">

**Пояснение:*** 
 
<img src="./graph/com_opt.jpg">

### Анализ результатов:
* Из графика метрики точности, можно сказать что лучший результат равный 89.07% достигается при использовании политики изменения темпа обучения - экспоненциальное затухание, что на 1.15% лучше, чем при фиксированном темпе обучение, и на 0.55% лучше, чем при пошаговом затухании.

* Из графика функции потерь, можно сказать что лучший результат 0.2313 равный достигается при использовании политики изменения темпа обучения - пошаговом затухании, что на 0.1748 меньше чем при фиксированном темпе обучения, и на 0.0631 меньше чем при экспоненциальном затухании.

* Оптимальный результат получен при использовании политики изменения темпа обучения - экспоненциальное затухание.
